import streamlit as st
from janome.tokenizer import Tokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import datetime

# ç¾åœ¨ã®å’Œæš¦å¹´ã‚’å–å¾—ï¼ˆä»¤å’Œï¼‰
current_year = datetime.datetime.now().year
reiwa_year = current_year - 2018
reiwa_tags = [f"#ä»¤å’Œ{reiwa_year - 2}å¹´ãƒ™ãƒ“ãƒ¼", f"#ä»¤å’Œ{reiwa_year - 1}å¹´ãƒ™ãƒ“ãƒ¼", f"#ä»¤å’Œ{reiwa_year}å¹´ãƒ™ãƒ“ãƒ¼"]

# å½¢æ…‹ç´ è§£æå™¨
tokenizer = Tokenizer()

# ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
stopwords = set([
    'ã“ã¨', 'ã‚‚ã®', 'ã“ã‚Œ', 'ãã‚Œ', 'ãŸã‚', 'ã‚ˆã†', 'ã•ã‚“', 'ã—ã¦', 'ã¾ã™', 'ã§ã™',
    'ã„ã‚‹', 'ã‚ã‚‹', 'ã™ã‚‹', 'ãªã‚‹', 'æœˆ', 'æ™‚', 'æ—¥', 'ã€œ', 'ãƒ»', 'â€»', 'ã‹ã‚‰',
    'ã”ã‚', 'ã¾ã§', 'æ¯æ—¥', 'åˆå‰', 'åˆå¾Œ', 'å†…å®¹', 'æ›œæ—¥', 'äºˆç´„', 'å—ä»˜',
    'å‚åŠ ', 'é–‹å‚¬', 'å­¦åŒº', 'ãŠè©±', 'å›£ä½“å', 'ã‚¤ãƒ™ãƒ³ãƒˆ', 'å¯¾è±¡å¹´é½¢', 'çµ‚äº†',
    'ã»ã‚Š', 'ã»ã‚Šã‚ªãƒ¼ãƒ—ãƒ³', 'ã‚ªãƒ¼ãƒ—ãƒ³', 'ã‚ªãƒ¼ãƒ—ãƒ³ãƒãƒ¼ãƒˆ', 'ãƒãƒ¼ãƒˆ', 'ã‚¨ãƒªã‚¢',
    'ã‚†ã‚Š', 'ã‹ã”', 'éƒ¨å±‹', 'ãŸã¡'
])

fixed_priority_keywords = set([
    'ã‚†ã‚Šã‹ã”', 'ã™ã¾ã„ã‚‹',
    'å­è‚²ã¦', 'å­è‚²ã¦æ”¯æ´', 'ç‘ç©‚åŒº', 'åå¤å±‹ãƒãƒ', 'åœ°åŸŸå­è‚²ã¦æ”¯æ´æ‹ ç‚¹',
    'å­è‚²ã¦å¿œæ´æ‹ ç‚¹', 'ã•ãã‚‰ã£ã“â™ª', 'å­è‚²ã¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã•ãã‚‰ã£ã“â™ª',
    'æœªå°±åœ’å…è¦ªå­', '0æ­³ãƒãƒ', '1æ­³ãƒãƒ', '2æ­³ãƒãƒ', '3æ­³ãƒãƒ',
    'æœªå°±åœ’å…', 'å­è‚²ã¦åºƒå ´', 'å­è‚²ã¦ç›¸è«‡', 'ãƒã‚¿ãƒ‹ãƒ†ã‚£', 'å¦Šå©¦',
    'ãƒ—ãƒ¬ãƒãƒ', 'æ–°ç±³ãƒãƒ', *reiwa_tags,
    'çµµæœ¬', 'æ‰‹éŠã³', 'åºƒå ´', 'ãƒ©ãƒ³ãƒ', 'è¦ªå­', 'ä¸€æ™‚é ã‹ã‚Š', 'ã‚¤ãƒ™ãƒ³ãƒˆ'
])

always_include_hashtags = list(dict.fromkeys([
    '#å­è‚²ã¦', '#å­è‚²ã¦æ”¯æ´', '#ç‘ç©‚åŒº', '#åå¤å±‹ãƒãƒ',
    '#åœ°åŸŸå­è‚²ã¦æ”¯æ´æ‹ ç‚¹', '#å­è‚²ã¦å¿œæ´æ‹ ç‚¹', '#ã•ãã‚‰ã£ã“â™ª',
    '#å­è‚²ã¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã•ãã‚‰ã£ã“â™ª', '#ç‘ç©‚åŒºãƒãƒ',
    '#ãƒ—ãƒ¬ãƒãƒ', '#æ–°ç±³ãƒãƒ', '#ãƒã‚¿ãƒ‹ãƒ†ã‚£', '#å¦Šå©¦',
    *reiwa_tags
]))

def is_valid_word(word):
    return (
        word not in stopwords and
        not re.fullmatch(r'\d+', word) and
        not re.fullmatch(r'[a-zA-Z]+', word) and
        not re.fullmatch(r'[\W_]+', word) and
        len(word) > 1
    )

def tokenize(text):
    tokens = tokenizer.tokenize(text)
    words = []
    for token in tokens:
        base = token.base_form
        pos = token.part_of_speech.split(',')[0]
        if (pos in ['åè©', 'å›ºæœ‰åè©']) and is_valid_word(base):
            words.append(base)
    return words

def extract_keywords(text, top_n=15):
    words = tokenize(text)
    if not words:
        return []

    vectorizer = TfidfVectorizer(
        tokenizer=lambda x: x, lowercase=False, ngram_range=(1, 2)
    )
    tfidf = vectorizer.fit_transform([words])
    scores = zip(vectorizer.get_feature_names_out(), tfidf.toarray()[0])
    sorted_words = sorted(scores, key=lambda x: x[1], reverse=True)

    fixed_in_text = [kw for kw in fixed_priority_keywords if kw in text]
    extracted = fixed_in_text[:]

    for word, score in sorted_words:
        if word not in extracted and len(extracted) < top_n:
            extracted.append(word)
    return extracted[:top_n]

def extract_org_hashtag(text):
    match = re.search(r'å›£ä½“å[:ï¼š]\s*([^\n\r]+)', text)
    if match:
        org_name = match.group(1).strip()
        clean_name = re.sub(r'[^\wã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾¥ãƒ¼â™¡â™¥ãƒ¼ãƒ»ï¼†A-Za-z0-9]', '', org_name)
        return '#' + clean_name
    return None

def extract_title_hashtag(text):
    match = re.search(r'ã€Œ\s*\d+æœˆ\s+(.+?)ã€ã‚’é–‹å‚¬ã—ã¾ã—ãŸ', text)
    if match:
        raw = match.group(1).strip()
        clean = re.sub(r'[^\wã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾¥ãƒ¼â™¡â™¥ãƒ¼ãƒ»ï¼†A-Za-z0-9]', '', raw)
        return '#' + clean
    return None

def generate_hashtags(text, selected_fixed_tags, top_n=15):
    keywords = extract_keywords(text, top_n)
    auto_tags = ['#' + re.sub(r'\s+', '', kw) for kw in keywords]

    title_tag = extract_title_hashtag(text)
    org_tag = extract_org_hashtag(text)
    for tag in [title_tag, org_tag]:
        if tag and tag not in auto_tags:
            auto_tags = [tag] + auto_tags

    fixed_tags = [tag for tag in selected_fixed_tags if tag not in auto_tags]
    return auto_tags + fixed_tags

# --- Streamlit UI ---
st.title("å­è‚²ã¦æŠ•ç¨¿å‘ã‘ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°è‡ªå‹•ç”Ÿæˆã‚¢ãƒ—ãƒª")

user_input = st.text_area("æŠ•ç¨¿å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š", height=200)

st.markdown("### å›ºå®šãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼š")
num_columns = 3
cols = st.columns(num_columns)
selected_fixed_tags = []
for i, tag in enumerate(always_include_hashtags):
    col = cols[i % num_columns]
    with col:
        if st.checkbox(tag, value=True, key=f"fixed_tag_{i}"):
            selected_fixed_tags.append(tag)

# ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ç”Ÿæˆ
if st.button("âœ… ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’ç”Ÿæˆ"):
    if user_input.strip():
        tags = generate_hashtags(user_input, selected_fixed_tags)
        tags = list(dict.fromkeys(tags))  # é‡è¤‡å‰Šé™¤
        st.session_state["post_text"] = user_input.strip()
        st.session_state["hashtags_selected"] = {tag: True for tag in tags}
    else:
        st.warning("æŠ•ç¨¿æ–‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

# --------------------------
# ç·¨é›†ã‚¨ãƒªã‚¢
# --------------------------
if "post_text" in st.session_state:
    st.markdown("### ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ç·¨é›†ï¼ˆãƒã‚§ãƒƒã‚¯ã‚’å¤–ã™ã¨å‰Šé™¤ã•ã‚Œã¾ã™ï¼‰")
    hashtags_selected = st.session_state.get("hashtags_selected", {})

    # ã™ã¹ã¦ã®ã‚¿ã‚°ã‚’ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹è¡¨ç¤º
    all_tags = list(hashtags_selected.keys())
    cols_edit = st.columns(3)
    for i, tag in enumerate(all_tags):
        col = cols_edit[i % 3]
        with col:
            checked = st.checkbox(tag, value=hashtags_selected.get(tag, True), key=f"edit_{tag}")
            hashtags_selected[tag] = checked

    # --- æ–°ã—ã„ã‚¿ã‚°ã®è¿½åŠ  ---
    with st.form(key="add_tag_form"):
        new_tag_input = st.text_input("æ–°ã—ã„ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’è¿½åŠ ï¼ˆ#ç„¡ã—ã§OKï¼‰", key="new_tag_input")
        submitted = st.form_submit_button("è¿½åŠ ")
        if submitted:
            new_tag = new_tag_input.strip()
            if new_tag:
                if not new_tag.startswith("#"):
                    new_tag = "#" + new_tag
                hashtags_selected[new_tag] = True  # è¿½åŠ ã—ãŸã‚‰å³ãƒã‚§ãƒƒã‚¯ON
            st.rerun()

    st.session_state["hashtags_selected"] = hashtags_selected

    # é¸æŠã•ã‚Œã¦ã„ã‚‹ã‚¿ã‚°ã ã‘ã‚’åæ˜ 
    final_tags = [tag for tag, selected in hashtags_selected.items() if selected]
    preview_text = st.session_state["post_text"] + "\n\n" + " ".join(final_tags)

    # --- ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æ•°ã®è¡¨ç¤º ---
    hashtag_count = len(final_tags)
    if hashtag_count < 20:
        st.markdown(f"<div style='background-color:#FFF3CD;color:#856404;padding:10px;border-radius:8px;'>âš ï¸ ç¾åœ¨ã®ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æ•°ï¼š<b>{hashtag_count}å€‹</b>ï¼ˆå°‘ãªã‚ï¼‰<br>20å€‹ä»¥ä¸Šã¤ã‘ã‚‹ã¨åŠ¹æœçš„ã§ã™ï¼</div>", unsafe_allow_html=True)
    elif hashtag_count > 25:
        st.markdown(f"<div style='background-color:#D1ECF1;color:#0C5460;padding:10px;border-radius:8px;'>â„¹ï¸ ç¾åœ¨ã®ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æ•°ï¼š<b>{hashtag_count}å€‹</b>ï¼ˆå¤šã‚ï¼‰<br>ã‚„ã‚„å¤šã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒåŠ¹æœã¯é«˜ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚</div>", unsafe_allow_html=True)
    else:
        st.markdown(f"<div style='background-color:#D4EDDA;color:#155724;padding:10px;border-radius:8px;'>âœ… ç¾åœ¨ã®ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æ•°ï¼š<b>{hashtag_count}å€‹</b>ï¼ˆã¡ã‚‡ã†ã©ã„ã„ï¼‰</div>", unsafe_allow_html=True)

    # --- ã‚³ãƒ”ãƒ¼æ©Ÿèƒ½ ---
    js_code = f"""
    <script>
    function copyToClipboard(text) {{
        navigator.clipboard.writeText(text).then(function() {{
            alert("ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ");
        }});
    }}
    </script>
    <div style="text-align: right; margin-bottom: -10px; margin-top: -5px;">
    <button onclick="copyToClipboard(`{preview_text}`)">ğŸ“‹ ã‚³ãƒ”ãƒ¼</button>
    </div>
    """
    st.markdown("### ğŸ‘€ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæŠ•ç¨¿æ–‡ï¼‹ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ï¼‰")
    st.components.v1.html(js_code, height=40)
    st.text_area("æŠ•ç¨¿æ–‡ï¼‹ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ï¼š", value=preview_text, height=180, disabled=True)
